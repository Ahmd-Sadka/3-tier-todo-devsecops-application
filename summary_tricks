 # use the following command to update the kubeconfig
aws eks update-kubeconfig --region us-east-1 --name zephyr-eks

## to create a remote state file
 --------------------------------------
  backend "s3" {
    bucket         = "your-terraform-state-bucket"
    key            = "eks-cluster/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
   dynamodb_table = "terraform-lock"
  }

terraform graph | dot -Tpng > graph.png && open graph.png # create terraform graph
------------------------------------------------

## when creating sg use create_before_destroy = true meta argument to avoid down time of the resources attached to the sg
------------------------------------------------

## aws backup Module

## to avoid using hardcoded account number you can use data source aws_caller_identity.current.account_id
   or you can use resources arn of the resources you want to back up

----------------------------------------------------------------
## Ansible playbook 
--------------------------------------

# note: not every thing should be automated, some things should be done manually like creating credentials even if its encrypted
# so that we can have a better control over the security of the system, so we can have a better control over the security of the system
# may be in future we can automate this process as well but for now we will keep it manual
# so we can have a better control over the security of the system

#--------------------------------------

##s3Module

# appling s3 module at first using the following command
terraform apply -target=module.s3
then we can remove it from the state file using the following command
terraform state rm module.s3
to prevent any one using terraform destroy command from destroying the s3 module and deleting the remote state file

or we can use 
lifecycle { prevent_destroy = true }

# then we can apply the rest of the resources using the following command
terraform apply 
--------------------------------------
# helm releases
## very important

after creating eks and node groupwe should initialize again to make helm can interact with our cluster in order to perfoem helm commands to install the helm charts

-----------------------------------
------------------

eksctl utils associate-iam-oidc-provider --region=us-east-1 --cluster=zephyr-eks --approve
2025-03-20 09:24:12 [ℹ]  will create IAM Open ID Connect provider for cluster "zephyr-eks" in "us-east-1"
2025-03-20 09:24:13 [✔]  created IAM Open ID Connect provider for cluster "zephyr-eks" in "us-east-1"

 eksctl create iamserviceaccount \
  --name ebs-csi-controller-sa \
  --namespace kube-system \
  --cluster zephyr-eks \
  --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
  --approve \
  --override-existing-serviceaccounts

  ---------------------------------------

  ## aws ebs csi driver
  first we need to get the security certificate from eks-cluster oidc provider
  
  then  tells AWS, “Hey, here’s my EKS cluster’s OIDC endpoint. Trust it.” to allow k8s pods (aws-ebs-csi-driver-pods) to use aws permissions securely

  resource "aws_iam_openid_connect_provider" "eks" {
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.eks.certificates[0].sha1_fingerprint]
  url             = aws_eks_cluster.my_cluster.identity[0].oidc[0].issuer
}

afetr that we need to Create a Trust Policy for the EBS CSI Driver
 writes a simple rule saying that only the Kubernetes service account named ebs-csi-controller-sa (in the kube-system namespace) can use a special AWS role.

 -----------------------------------

 kubectl patch ds prometheus-prometheus-node-exporter --type "json" -p '[{"op": "remove", "path" : "/spec/template/spec/containers/0/volumeMounts/2/mountPropagation"}]'

kubectl --namespace eye get secrets show-time-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo
admin

kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 -d

